import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

train = pd.read_csv('cr_loan2.csv')
original = pd.read_csv('cr_loan2.csv')

train.head()

print(train.dtypes)

    person_age                      int64
    person_income                   int64
    person_home_ownership          object
    person_emp_length             float64
    loan_intent                    object
    loan_grade                     object
    loan_amnt                       int64
    loan_int_rate                 float64
    loan_status                     int64
    loan_percent_income           float64
    cb_person_default_on_file      object
    cb_person_cred_hist_length      int64
    dtype: object

train.columns
    Index(['person_age', 'person_income', 'person_home_ownership',
           'person_emp_length', 'loan_intent', 'loan_grade', 'loan_amnt',
           'loan_int_rate', 'loan_status', 'loan_percent_income',
           'cb_person_default_on_file', 'cb_person_cred_hist_length'],
          dtype='object')
   
# 12 columns are present
 
train.isnull().sum()
    person_age                       0
    person_income                    0
    person_home_ownership            0
    person_emp_length              895
    loan_intent                      0
    loan_grade                       0
    loan_amnt                        0
    loan_int_rate                 3116
    loan_status                      0
    loan_percent_income              0
    cb_person_default_on_file        0
    cb_person_cred_hist_length       0

# So loan interest rate and employment length are the variables for which we need to do impuatation.
    
# Distribution of loan interest rate
plt.hist(train['loan_int_rate'], bins='auto', alpha=0.7)
plt.xlabel('Loan interest rate')
plt.show()
￼
# 7.5 having the highest frequency is the median of interest rate distribution and the distribution is not unimodal  nor a normal distributionn so taking mean to impute th missig
values might no￼￼￼￼t be the appropriate approach.

plt.scatter(train['person_income'], train['person_age'], alpha=0.5)
plt.xlabel('Person income')
plt.ylabel('person age')
plt.show()

#Person age above 100 is not possible so we have to remove those data points or replace them with suitable value.
#Plot describe some form of linear relationship with some outlier

#Relationship among variables using crosstable
pd.crosstab(train['loan_intent'], train['loan_status'], margins=True)

    loan_status            0     1    All
    loan_intent                          
    DEBTCONSOLIDATION   3722  1490   5212
    EDUCATION           5342  1111   6453
    HOMEIMPROVEMENT     2664   941   3605
    MEDICAL             4450  1621   6071
    PERSONAL            4423  1098   5521
    VENTURE             4872   847   5719
    All                25473  7108  32581

print(pd.crosstab(train['loan_status'],train['person_home_ownership'],values=train['person_emp_length'], aggfunc='max'))

    person_home_ownership  MORTGAGE  OTHER   OWN   RENT
    loan_status                                        
    0                         123.0   24.0  31.0   41.0
    1                          34.0   11.0  17.0  123.0
    
indices = train[train['person_emp_length'] > 60].index
train = train.drop(indices)
indices = train[train['person_age'] > 100].index
train = train.drop(indices)

print(pd.crosstab(train['loan_status'],train['person_home_ownership'], values=train['person_emp_length'], aggfunc=['min','max']))

                               min                      max                  
    person_home_ownership MORTGAGE OTHER  OWN RENT MORTGAGE OTHER   OWN  RENT
    loan_status                                                              
    0                          0.0   0.0  0.0  0.0     38.0  24.0  31.0  41.0
    1                          0.0   0.0  0.0  0.0     34.0  11.0  17.0  27.0
                  
                  
print(train.columns[train.isnull().any()])
print(train[train['person_emp_length'].isnull()].head())
train['person_emp_length'].fillna((train['person_emp_length'].median()), inplace=True)
n, bins, patches = plt.hist(train['person_emp_length'], bins='auto', color='blue')
plt.xlabel("Person Employment Length")
plt.show()                  
                  
# Now we find the correlation between loan interest rate and loan grade to fill the missing values of loan int rate
pd.crosstab(train['loan_grade'], train['loan_status'], values=train['loan_int_rate'], aggfunc='mean', margins=True)

       loan_status          0          1        All
        loan_grade                                  
        A             7.303311   7.547595   7.327732
        B            10.987726  11.035864  10.995555
        C            13.464246  13.460852  13.463542
        D            15.391206  15.340827  15.361250
        E            16.992935  17.018424  17.009455
        F            18.793750  18.530400  18.609159
        G            19.160000  20.270345  20.251525
        All          10.436168  13.059749  11.011677
                  
# So the correlation is clearly visible and we can go ahead imputing the missing values of interest rate on the basis of the loan grade

for i in ['A', 'B', 'C', 'D', 'E', 'F', 'G']:
    print(i)
    train['loan_int_rate'][train['loan_grade'] == i] = train['loan_int_rate'][train['loan_grade'] == i].fillna(train['loan_int_rate'][train['loan_grade'] == i].mean())
                  
# Now our data set is complete with no null values and no absurd values  

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_fscore_support
from sklearn.metrics import confusion_matrix

# Separate categorical and numerical variable
train_cat = train.select_dtypes(include=['object'])
train_num = train.select_dtypes(exclude=['object'])

train_cat = pd.get_dummies(train_cat)
train = pd.concat([train_num, train_cat], axis=1)
train.dtypes

X = train.drop('loan_status', axis=1)
y = train['loan_status']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, stratify= y)

model = LogisticRegression(solver='lbfgs')

model.fit(X_train, y_train)
pred_proba = model.predict_proba(X_test)
preds_df = pd.DataFrame(pred_proba[:, 1][:5], columns = ['prob_default'])
actual = y_test.head(5)
print(pd.concat([actual.reset_index(drop=True), preds_df], axis=1))

preds_df = pd.DataFrame(pred_proba[:, 1], columns = ['prob_default'])
preds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.5 else 0)
preds_df['loan_status'].value_counts()

model.score(X_test, y_test)
target_names = ['Non-Default', 'Default']
classification_report(y_test, preds_df['loan_status'], target_names=target_names)
#Recall = true positive predicte out of actual positives present
#Precision = true positive predicted out of all positive predicted

#Calculating Roc-Auc Score
fallout, sensitivity, thresholds = roc_curve(y_test, preds_df['prob_default'])
plt.plot(fallout, sensitivity)
plt.plot([0,1], [0,1], linestyle = '--')
auc = roc_auc_score(y_test, preds_df['prob_default'])
plt.show()

#applying RandomForest

from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestClassifier
#Create a Gaussian Classifier
clf=RandomForestClassifier(n_estimators=20)

#Train the model using the training sets y_pred=clf.predict(X_test)
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=5, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
clf.feature_importances_

feature_imp= pd.Series(clf.feature_importances_,index=X_train.columns).sort_values(ascending=False)
feature_imp

%matplotlib inline
# Creating a bar plot

# Add labels to your graph
from matplotlib.pyplot import figure
figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')
sns.barplot(x=feature_imp, y=feature_imp.index)
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.legend()
plt.show()

from sklearn.metrics import precision_recall_fscore_support as score
precision, recall, fscore, support = score(y_test, y_pred)

print('precision: {}'.format(precision))
print('recall: {}'.format(recall))
print('fscore: {}'.format(fscore))
print('support: {}'.format(support))

precision: [0.92634379 0.95864662]
recall: [0.99136154 0.71755188]
fscore: [0.95775049 0.82076041]
support: [10187  2843]

